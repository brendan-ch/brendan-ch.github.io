---
title: "Thoughts on Taming Silicon Valley by Gary Marcus"
layout: post
date: 2026.01.30
---

*Taming Silicon Valley* by Gary Marcus does a pretty good job of summarizing the AI industry. As a book which came out in 2024, I was surprised how well the fundamentals held up reading it in 2026.

I think for the most part, the flaws of generative AI are obvious by now, and the book confirmed what I already knew. Hallucinations are still prevalent from the latest models like GPT-5[^1]. Gary Marcus has been spot on with many his predictions for the industry; for instance, he predicted that GPT-5 would be a flop, with the usual reasoning errors, and additional backlash from users.

The most valuable part of the book was the discussion on politics and rhetoric. The playbook of Silicon Valley is to overhype, underdeliver and move recklessly, while downplaying the need for regulation to the public and governments internationally.

On overhyping and underdelivering, Microsoft claimed in 2023[^2] that "sparks of AGI" had been achieved with GPT-4, and Altman claimed around the same time that "AGI had been achieved internally"[^3]; neither has actually come to fruition. Within the software engineering bubble, LLMs receive even greater hype due to the outsized impact that they have had; coding just happens to be a relevant task for a statistical prediction machine. There have been impressive feats of engineering built using AI agents[^4], but the underlying technology has also generated hilariously bad slop[^5] without a human in the loop. Perhaps I'm stating the obvious, but a truly smart coding agent would have a authoritative model of the codebase and truly understand what an error means. That's *definitely* not what we have right now.

On manipulating public opinion, AI labs tend to emphasize the powerfulness of models while downplaying the *actual* negative consequences we're seeing today. AI leaders pretending we're close to an AGI which will make us extinct[^6], in the style of science fiction, is a tactic designed to showcase power. On the other hand, the labs have not addressed the real-world consequences of misinformation leading to election interference[^7], as well as non-consensual deepfakes and child sexual abuse material. Government manipulation by big tech in the U.S. was present back then[^8], and has become even more prevalent now with the Trump administration[^9].

The book isn't anti-AI, and neither am I. Going a step further, I'm not anti-LLMs, either, and I think they bring immense value to the software engineering space. What this book does is call out what's happening *around* this technology. Going back a decade, the original intent of social media may truly have been to create a global "town square" or to connect friends and family. Money has corrupted that promise and turned the social media premise into a web of targeted advertising and engagement-driven algorithms, laying the foundation for misinformation and other consequences. A repeat is inevitable for AI if the current status quo is maintained.

[^1]: [Example by Colin Fraser on X](https://x.com/colin_fraser/status/1953668411029909892)
[^2]: [Sparks of Artificial General Intelligence: Early experiments with GPT-4](https://arxiv.org/abs/2303.12712)
[^3]: Turns out he [backtracked his statement](https://www.the-independent.com/tech/chatgpt-ai-agi-sam-altman-openai-b2419449.html) to say he was "just memeing".
[^4]: [Gas Town](https://github.com/steveyegge/gastown) by Steve Yegge is impressive, although I wonder how cost-effective running 20-30 Claude instances will be for real-world cases.
[^5]: My favorite rabbit hole while looking up references today has to be [this](https://github.com/dotnet/runtime/pull/115762): a pull request on the .NET codebase generated by GitHub Copilot, filled with hilarious comments.
[^6]: See reporting by [Kevin Roose for the NYT](https://www.nytimes.com/2023/05/30/technology/ai-threat-warning.html) and the [actual statement](https://aistatement.com/#open-letter) itself; notice how the statement itself is just one sentence long.
[^7]: This might have actually happened with the [Slovak case](https://misinforeview.hks.harvard.edu/article/beyond-the-deepfake-hype-ai-democracy-and-the-slovak-case/).
[^8]: Brendan Bordelon did an [interesting piece](https://www.politico.com/news/2023/12/03/congress-ai-fellows-tech-companies-00129701) for Politico on this.
[^9]: [Trump's "AI Action Plan"](https://www.theatlantic.com/technology/archive/2025/07/donald-trump-ai-action-plan/683647/) essentially lets AI companies lead the way; it encourages a "try-first" culture for AI where companies are encouraged to release products first and *then* determine risk.
